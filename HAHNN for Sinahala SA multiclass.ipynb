{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HAHNN for Sinahala SA multiclass.ipynb","provenance":[{"file_id":"1Ev_U1DIaobbu2aaRBjj1c2AOm5RoIvIp","timestamp":1597823612243},{"file_id":"1LH7xLroO6QWO9dC6Hipn7xHYxVchJiUt","timestamp":1591293377462}],"collapsed_sections":["4_pgmoOw4gkH","5exIp4egmwwA","JLle9TkY1T_X","voCjs6ewiSoF","QTR1_43CqGSW","Iu_O63OD2h8P","724247Q4phPE","qsTKHWjKqXmo","obo_fTOvUsi8","D9Zcz1wv0wf3","qOZg-uB4HV1H"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"EjNTyCbkZcKM","colab_type":"text"},"source":["# Hierarchical Attentional Hybrid Neural Networks for Document Classification\n","\n","Document classification is a challenging task with important applications. Deep learning approaches to the problem have gained much attention. Despite the progress, the proposed models do not incorporate the knowledge of the document structure in the architecture efficiently and not take into account the contexting dependent importance of words and sentences. In this paper, we propose a new approach based on convolutional neural networks, gated recurrent units and attention mechanisms for document classification tasks. The datasets IMDB Movie Reviews and Yelp were used in experiments. The proposed method improves the results of current attention-based approaches\n","\n","Please, cite:\n","\n","Abreu, J., Fred, L., Macêdo, D., & Zanchettin, C. (2019). Hierarchical Attentional Hybrid Neural Networks for Document Classification. arXiv preprint arXiv:1901.06610."]},{"cell_type":"code","metadata":{"id":"leRsyqFIcd_f","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1598537534880,"user_tz":-330,"elapsed":50633,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"outputId":"65f5730e-bda1-4b95-e1cf-d95085434e8b"},"source":["# Uncomment following at first run\n","\n","!pip install tensorflow==1.15.2\n","!pip install q keras==2.3.1\n","\n","# !pip -q install gensim\n","# # !python -m spacy download en_core_web_md\n","# !pip -q install paramiko"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==1.15.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/d9/fd234c7bf68638423fb8e7f44af7fcfce3bcaf416b51e6d902391e47ec43/tensorflow-1.15.2-cp36-cp36m-manylinux2010_x86_64.whl (110.5MB)\n","\u001b[K     |████████████████████████████████| 110.5MB 99kB/s \n","\u001b[?25hCollecting tensorflow-estimator==1.15.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n","\u001b[K     |████████████████████████████████| 512kB 49.4MB/s \n","\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 46.9MB/s \n","\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.8.1)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.18.5)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.35.1)\n","Collecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.31.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.1.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (3.12.4)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (3.3.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.2.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.12.1)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.8.1)\n","Collecting keras-applications>=1.0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 7.5MB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.1.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.15.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (49.6.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.2.2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.7.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.1.0)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=3e1256c49506803609155cdaadaadeacfaa295a2ce69f11603805bde22eea859\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","Installing collected packages: tensorflow-estimator, tensorboard, gast, keras-applications, tensorflow\n","  Found existing installation: tensorflow-estimator 2.3.0\n","    Uninstalling tensorflow-estimator-2.3.0:\n","      Successfully uninstalled tensorflow-estimator-2.3.0\n","  Found existing installation: tensorboard 2.3.0\n","    Uninstalling tensorboard-2.3.0:\n","      Successfully uninstalled tensorboard-2.3.0\n","  Found existing installation: gast 0.3.3\n","    Uninstalling gast-0.3.3:\n","      Successfully uninstalled gast-0.3.3\n","  Found existing installation: tensorflow 2.3.0\n","    Uninstalling tensorflow-2.3.0:\n","      Successfully uninstalled tensorflow-2.3.0\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.2 tensorflow-estimator-1.15.1\n","Collecting q\n","  Downloading https://files.pythonhosted.org/packages/53/bc/51619d89e0bd855567e7652fa16d06f1ed36a85f108a7fe71f6629bf719d/q-2.6-py2.py3-none-any.whl\n","Collecting keras==2.3.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n","\u001b[K     |████████████████████████████████| 378kB 6.0MB/s \n","\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (2.10.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.4.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.1.2)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.15.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.0.8)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.18.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (3.13)\n","Installing collected packages: q, keras\n","  Found existing installation: Keras 2.4.3\n","    Uninstalling Keras-2.4.3:\n","      Successfully uninstalled Keras-2.4.3\n","Successfully installed keras-2.3.1 q-2.6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"K3T5VpzOfqkC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1598537534883,"user_tz":-330,"elapsed":50605,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"outputId":"26f324b9-0f28-4bba-9c17-224462244296"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IFyTXuEK9TJ3","colab_type":"text"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"0ItSeCYVNr2Q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598537536860,"user_tz":-330,"elapsed":52539,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}},"outputId":"32bb96da-de92-47c8-a7b0-369b5ac13475"},"source":["import datetime, pickle, os, codecs, re, string\n","import json\n","import random\n","import numpy as np\n","import keras\n","\n","from keras.models import *\n","from keras.layers import *\n","from keras.optimizers import *\n","from keras.callbacks import *\n","from keras import regularizers\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras import backend as K\n","from keras.utils import CustomObjectScope\n","from keras.engine.topology import Layer\n","from numpy import array\n","from numpy import argmax\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.model_selection import train_test_split,cross_val_score, cross_val_predict, KFold, GridSearchCV\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix,precision_recall_fscore_support\n","\n","\n","#\n","from keras.engine import InputSpec\n","\n","from keras import initializers\n","\n","import pandas as pd\n","from tqdm import tqdm\n","\n","import string\n","from spacy.lang.en import English\n","import gensim, nltk, logging\n","\n","# from nltk.corpus import stopwords\n","# from nltk import tokenize\n","# from nltk.stem import SnowballStemmer\n","\n","# nltk.download('punkt')\n","# nltk.download('stopwords')\n","\n","from sklearn.manifold import TSNE\n","\n","import matplotlib.pyplot as plt\n","import en_core_web_sm\n","\n","from IPython.display import HTML, display\n","\n","import tensorflow as tf\n","\n","from numpy.random import seed\n","from tensorflow import set_random_seed\n","os.environ['PYTHONHASHSEED'] = str(1024)\n","# tf.random.set_seed(1024)\n","set_random_seed(1024)\n","seed(1024)\n","np.random.seed(1024)\n","random.seed(1024)\n","\n","import os\n","import sys\n","from numpy import array\n","from numpy import argmax\n","\n","from gensim.models.keyedvectors import KeyedVectors\n","import pickle"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"OCCJXPA9Nm6C","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598537536865,"user_tz":-330,"elapsed":52529,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}}},"source":["dataset = \"imdb\" #@param [\"yelp\", \"imdb\"]\n","\n","word_embedding_type = \"pre_trained\" #@param [\"from_scratch\", \"pre_trained\"]\n","word_vector_model = \"fasttext\" #@param [\"fasttext\"]\n","rnn_type = \"LSTM\" #@param [\"LSTM\", \"GRU\"]\n","learning_rate = 0.001\n","epochs = 8\n","batch_size = 64"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UQgQvVSsI1Hc","colab_type":"text"},"source":["# Folder Paths"]},{"cell_type":"code","metadata":{"id":"bh4bfoO1oiyw","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598537536866,"user_tz":-330,"elapsed":52519,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}}},"source":["\n","# folder_path = '/content/drive/My Drive/University/FYP/Sentiment Analysis/Implementation/'\n","folder_path = '/content/drive/My Drive/FYP/Sentiment Analysis/Implementation/'\n","HANN_path = folder_path + 'Sentiment Analysis/HAANN/Sinhala/'\n","datasets_path = folder_path + 'corpus/new/preprocess_from_isuru/'\n","# word_embedding_path = HANN_path + 'word_embedding/'\n","# fasttext_path = word_embedding_path + 'fasttext/'\n","\n","lankadeepa_data_path = datasets_path + 'lankadeepa_tagged_comments.csv'\n","gossip_lanka_data_path = datasets_path + 'gossip_lanka_tagged_comments.csv'\n","\n","lankadeepa_data_binary = folder_path + 'corpus/analyzed/lankadeepa_tagged_1.csv'\n","\n","word_embedding_path = HANN_path + 'word_embedding/'\n","\n","KEYED_VECTORS_PATH = folder_path + \"word_embedding/fasttext/source2_data_from_gosspiLanka_and_lankadeepa/300/keyed_vectors/keyed.kv\"\n","EMBEDDING_MATRIX_PATH = HANN_path + 'word_embedding/fasttext_lankadeepa_gossip_300_5'\n","\n","SAVED_MODEL_DIR = HANN_path + 'saved_models'\n","SAVED_MODEL_FILENAME = 'model_sinhala_multiclass.h5'"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Dw1IN7EO9DL","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598537536866,"user_tz":-330,"elapsed":52508,"user":{"displayName":"Lahiru Senevirathne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRzF3KfxeuLmDoDB90oNXQ5_ivLn8-Ar4kecdc=s64","userId":"12627549003426465017"}}},"source":["sys.path.insert(0, HANN_path)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4_pgmoOw4gkH","colab_type":"text"},"source":["# Load dataset"]},{"cell_type":"code","metadata":{"id":"3LpvH92D67Z8","colab_type":"code","colab":{}},"source":["lankadeepa_data = pd.read_csv(lankadeepa_data_path)[:9059]\n","gossipLanka_data = pd.read_csv(gossip_lanka_data_path)\n","gossipLanka_data = gossipLanka_data.drop(columns=['Unnamed: 3'])\n","\n","all_data = pd.concat([lankadeepa_data,gossipLanka_data], ignore_index=True)\n","all_data.to_csv()\n","# all_data.shape\n","\n","# lankadeepa_data  = pd.read_csv(all_data,\",\")\n","# lankadeepa_data['comment'][5]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xRPTmxQWD7U_","colab_type":"code","colab":{}},"source":["def preprocess_text(data):\n","  comments = data['comment']\n","  labels = data['label']\n","\n","  #preprocess comments\n","  comments_list = []\n","\n","  for comment in comments:\n","    try:\n","      sentences = comment.split('.')\n","\n","      filtered_list = [x.strip() for x in sentences if len(x) > 1]\n","\n","      comments_list.append(filtered_list)\n","    except:\n","      comments_list.append([comment])\n","      continue\n","\n","  #preprocess labels\n","\n","  values = array(labels)\n","\n","  # integer encode\n","  label_encoder = LabelEncoder()\n","  integer_encoded = label_encoder.fit_transform(values)\n","  # binary encode\n","  onehot_encoder = OneHotEncoder(sparse=False)\n","  integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n","  onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n","\n","  # inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n","  # print(inverted)\n","  \n","  #convert to int\n","  labels = np.array(onehot_encoded).astype(np.uint8)\n","\n","  return comments_list,labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4nUcGPybFSRT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1597994331052,"user_tz":-330,"elapsed":2282,"user":{"displayName":"dataPirates fyp","photoUrl":"","userId":"14020399926305222994"}},"outputId":"c5b03a43-eee8-4ac8-dfc7-6ddf63944882"},"source":["X,Y = preprocess_text(all_data)\n","Y[:5]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 0, 1, 0],\n","       [0, 0, 1, 0],\n","       [0, 1, 0, 0],\n","       [1, 0, 0, 0],\n","       [1, 0, 0, 0]], dtype=uint8)"]},"metadata":{"tags":[]},"execution_count":97}]},{"cell_type":"markdown","metadata":{"id":"5exIp4egmwwA","colab_type":"text"},"source":["# Text preprocessing"]},{"cell_type":"code","metadata":{"id":"zY1TBBBbqEbp","colab_type":"code","trusted":true,"colab":{}},"source":["def clean_str(string):\n","    string = re.sub(r\"\\'s\", \" \\'s\", string)\n","    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n","    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n","    string = re.sub(r\"\\'re\", \" \\'re\", string)\n","    string = re.sub(r\"\\'d\", \" \\'d\", string)\n","    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n","    string = re.sub(r\",\", \" , \", string)\n","    string = re.sub(r\"!\", \" ! \", string)\n","    string = re.sub(r\"\\(\", \" \\( \", string)\n","    string = re.sub(r\"\\)\", \" \\) \", string)\n","    string = re.sub(r\"\\?\", \" \\? \", string)\n","    string = re.sub(r\"\\s{2,}\", \" \", string)\n","\n","    cleanr = re.compile('<.*?>')\n","\n","    # string = re.sub(r'\\d+', '', string)\n","    string = re.sub(cleanr, '', string)\n","    # string = re.sub(\"'\", '', string)\n","    # string = re.sub(r'\\W+', ' ', string)\n","    string = string.replace('_', '')\n","\n","\n","    return string.strip().lower()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JLle9TkY1T_X","colab_type":"text"},"source":["# Use pre-trained word embeddings"]},{"cell_type":"code","metadata":{"id":"BYq1uFsGIPYs","colab_type":"code","colab":{}},"source":["def load_subword_embedding_300d(word_index):\n","    print('load_subword_embedding...')\n","    embeddings_index = {}\n","    f = codecs.open(word_embedding_path + \"wiki-news-300d-1M-subword.vec\", encoding='utf-8')\n","    for line in tqdm(f):\n","        values = line.rstrip().rsplit(' ')\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        embeddings_index[word] = coefs\n","    f.close()\n","    print('found %s word vectors' % len(embeddings_index))\n","    \n","    #embedding matrix\n","    print('preparing embedding matrix...')\n","    words_not_found = []\n","    \n","    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n","    \n","    for word, i in word_index.items():\n","        embedding_vector = embeddings_index.get(word)\n","        if (embedding_vector is not None) and len(embedding_vector) > 0:\n","            \n","            embedding_matrix[i] = embedding_vector\n","        else:\n","            words_not_found.append(word)\n","    \n","    return embedding_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0NaQr7agONxC","colab_type":"code","colab":{}},"source":["def load_fasttext_embedding_300d_new(word_index):\n","    print('load_subword_embedding...')\n","    embeddings_index = {}\n","    f = codecs.open(word_embedding_path + \"wiki-news-300d-1M-subword.vec\", encoding='utf-8')\n","    for line in tqdm(f):\n","        values = line.rstrip().rsplit(' ')\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        embeddings_index[word] = coefs\n","    f.close()\n","    print('found %s word vectors' % len(embeddings_index))\n","    \n","    #embedding matrix\n","    print('preparing embedding matrix...')\n","    words_not_found = []\n","    \n","    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n","    \n","    for word, i in word_index.items():\n","        embedding_vector = embeddings_index.get(word)\n","        if (embedding_vector is not None) and len(embedding_vector) > 0:\n","            \n","            embedding_matrix[i] = embedding_vector\n","        else:\n","            words_not_found.append(word)\n","    \n","    return embedding_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oLodIgWw14kC","colab_type":"code","trusted":true,"colab":{}},"source":["def load_fasttext_embedding_300d(word_index):\n","\n","    keyed_vectors = KeyedVectors.load(KEYED_VECTORS_PATH, mmap='r')\n","\n","    embeddings_index = dict()\n","    for word, vocab_obj in keyed_vectors.vocab.items():\n","      embeddings_index[word]=keyed_vectors[word]\n","\n","    # create a weight matrix for words in training docs\n","    words_not_found = []\n","    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n","    for word, i in word_index.items():\n","      embedding_vector = embeddings_index.get(word)\n","      if (embedding_vector is not None) and len(embedding_vector) > 0:\n","        embedding_matrix[i] = embedding_vector\n","    else:\n","      words_not_found.append(word)\n","\n","    pickle.dump(embedding_matrix, open(EMBEDDING_MATRIX_PATH, 'wb'))\n","    return embedding_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U1qSdjPJ5J1-","colab_type":"code","colab":{}},"source":["def pickle_load_embedding_matrix():\n","  f = open(EMBEDDING_MATRIX_PATH, 'rb')\n","  embedding_matrix= np.array(pickle.load(f))\n","  return embedding_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"voCjs6ewiSoF","colab_type":"text"},"source":["# Plot word embedding chart"]},{"cell_type":"code","metadata":{"id":"0rXXQmEMibaF","colab_type":"code","trusted":true,"colab":{}},"source":["# def plot_with_labels(low_dim_embs, labels, filename= word_embedding_path + 'tsne.png'):    \n","#     plt.figure(figsize=(18, 18))\n","#     for i, label in enumerate(labels):\n","#         x, y = low_dim_embs[i, :]\n","#         plt.scatter(x, y)\n","#         plt.annotate(label,\n","#                  xy=(x, y),\n","#                  xytext=(5, 2),\n","#                  textcoords='offset points',\n","#                  ha='right',\n","#                  va='bottom')\n","#     plt.savefig(filename)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QTR1_43CqGSW","colab_type":"text"},"source":["# Normalize texts"]},{"cell_type":"code","metadata":{"id":"ePLZEjNxqPKe","colab_type":"code","trusted":true,"colab":{}},"source":["nlp = en_core_web_sm.load()\n","\n","\n","puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n"," '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n"," '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n"," '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n"," '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', '#', '—–']\n","\n","\n","def clean_puncts(x):\n","    x = str(x)\n","    for punct in puncts:\n","        x = x.replace(punct, f' {punct} ')\n","    return x\n","\n","def remove_stopwords(text):\n","    text = str(text)    \n","    ## Convert words to lower case and split them\n","    text = text.lower().split()\n","    \n","    ## Remove stop words\n","    stops = set(stopwords.words(\"english\"))\n","    text = [w for w in text if not w in stops and len(w) >= 3]\n","    text = \" \".join(text)\n","    \n","    return text\n","\n","\n","def normalize(text):\n","    text = text.lower().strip()\n","    doc = nlp(text)\n","    filtered_sentences = []\n","    for sentence in doc.sents:                    \n","        sentence = clean_puncts(sentence)\n","        sentence = clean_str(sentence)            \n","        #sentence = remove_stopwords(sentence)                \n","        filtered_sentences.append(sentence)\n","    return filtered_sentences"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Iu_O63OD2h8P","colab_type":"text"},"source":["# Training word embeddings"]},{"cell_type":"code","metadata":{"id":"HXyS2m7C2pZ7","colab_type":"code","trusted":true,"colab":{}},"source":["def create_fasttext(embed_dim, data):\n","    \n","    filename = fasttext_path + 'fasttext_model.txt'\n","    \n","    if not os.path.isfile(filename):    \n","        print('create_fasttext...')\n","        sent_lst = []\n","\n","        for doc in data['text']:\n","            doc = clean_str(doc)\n","            sentences = nltk.tokenize.sent_tokenize(doc)\n","            for sent in sentences:\n","                word_lst = [w for w in nltk.tokenize.word_tokenize(sent) if w.isalnum()]\n","                sent_lst.append(word_lst)\n","\n","\n","        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","\n","        fasttext_model = gensim.models.FastText(\n","            word_ngrams=1,\n","            sentences=sent_lst, \n","            size = embed_dim, \n","            workers=os.cpu_count(), \n","            window = 1)\n","        fasttext_model.save(filename)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"724247Q4phPE","colab_type":"text"},"source":["# Load datasets\n","\n","**1)** Yelp Dataset with 500k reviews\n","\n","**2)** IMDb with 50k reviews"]},{"cell_type":"code","metadata":{"id":"0Owr7XARpnO7","colab_type":"code","trusted":true,"colab":{}},"source":["def load_data_yelp(path, train_ratio=1, size=400000):\n","    vector_dim = 200\n","    dim = 5\n","    \n","    with open(path) as f:\n","        reviews=f.read().strip().split(\"\\n\")\n","        \n","    df = pd.DataFrame([json.loads(review) for review in reviews])        \n","\n","    text_tokens = []\n","    for row in tqdm(df['text']):    \n","        text_tokens.append(normalize(row))  \n","    \n","    df['text_tokens'] = text_tokens    \n","    del text_tokens\n","    \n","    vector_dim = 200     \n","    if word_embedding_type is 'from_scratch':\n","        create_fasttext(vector_dim, df)\n","        \n","    ###\n","    \n","    \n","    X = df['text_tokens'].values\n","    Y = pd.get_dummies(df['stars']).values\n","    \n","    return (X, Y)\n","\n","def load_data_imdb(path, size=49000, train_ratio=1):    \n","    df = pd.read_csv(path, nrows=size, usecols=['text', 'sentiment'])        \n","    \n","    vector_dim = 200 \n","    \n","    if word_embedding_type is 'from_scratch':\n","        # Fasttext\n","        create_fasttext(vector_dim, df)\n","    \n","    ### \n","    \n","    text_tokens = []\n","    for row in tqdm(df['text']):    \n","        text_tokens.append(normalize(row))  \n","    \n","    df['text_tokens'] = text_tokens\n","    \n","    del text_tokens\n","    ###\n","    \n","    X = df['text_tokens'].values\n","    Y = pd.get_dummies(df['sentiment']).values\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qsTKHWjKqXmo","colab_type":"text"},"source":["# Attention Layer\n","Check [(Bahdanau et al., 2015)](https://arxiv.org/pdf/1409.0473.pdf)"]},{"cell_type":"code","metadata":{"id":"C_HrvPCJ9HYs","colab_type":"code","trusted":true,"colab":{}},"source":["class Attention(Layer):\n","    def __init__(self, **kwargs):\n","        self.init = initializers.get('normal')\n","        self.supports_masking = True\n","        self.attention_dim = 50\n","        super(Attention, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        assert len(input_shape) == 3\n","        self.W = K.variable(self.init((input_shape[-1], 1)))\n","        self.b = K.variable(self.init((self.attention_dim, )))\n","        self.u = K.variable(self.init((self.attention_dim, 1)))\n","        # self.trainable_weights = [self.W, self.b, self.u]\n","        self.trainable_weights.append([self.W, self.b, self.u]) \n","        super(Attention, self).build(input_shape)\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return mask\n","        \n","\n","    def call(self, x, mask=None):\n","        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n","        ait = K.dot(uit, self.u)\n","        ait = K.squeeze(ait, -1)\n","        ait = K.exp(ait)\n","\n","        if mask is not None:\n","            ait *= K.cast(mask, K.floatx())\n","            \n","        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n","        ait = K.expand_dims(ait)\n","        weighted_input = x * ait\n","        output = K.sum(weighted_input, axis=1)\n","        return output\n","\n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0], input_shape[-1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lmmw7Qfvqx-f","colab_type":"text"},"source":["# Model architecture"]},{"cell_type":"code","metadata":{"id":"jHLATdJqEIiN","colab_type":"code","trusted":true,"colab":{}},"source":["# !rm -rf saved_models\n","# !mkdir saved_models"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V6bzUS1dqftf","colab_type":"code","trusted":true,"colab":{}},"source":["class HAHNetwork():\n","    def __init__(self):\n","        self.model = None\n","        self.MAX_SENTENCE_LENGTH = 0\n","        self.MAX_SENTENCE_COUNT = 0\n","        self.VOCABULARY_SIZE = 0\n","        self.word_embedding = None\n","        self.model = None\n","        self.word_attention_model = None\n","        self.tokenizer = None\n","        self.class_count = 2\n","     \n","    def build_model(self, n_classes=2, embedding_dim=200, embeddings_path=False):\n","        \n","        l2_reg = regularizers.l2(0.001)\n","        \n","        embedding_weights = np.random.normal(0, 1, (len(self.tokenizer.word_index) + 1, embedding_dim))\n","        \n","        if embeddings_path is not None:\n","\n","            if word_embedding_type is 'from_scratch':\n","                # FastText\n","                filename = fasttext_path + 'fasttext_model.txt'                \n","                model =  gensim.models.FastText.load(filename)\n","\n","                embeddings_index = model.wv                    \n","                embedding_matrix = np.zeros( ( len(self.tokenizer.word_index) + 1, embedding_dim) )\n","                for word, i in self.tokenizer.word_index.items():\n","                    try:\n","                        embedding_vector = embeddings_index[word]\n","                        if embedding_vector is not None:\n","                            embedding_matrix[i] = embedding_vector\n","                    except Exception as e:\n","                        #print(str(e))\n","                        continue\n","\n","\n","            else:                \n","                embedding_dim = 300\n","                # embedding_matrix = load_subword_embedding_300d(self.tokenizer.word_index)\n","                embedding_matrix = load_fasttext_embedding_300d(self.tokenizer.word_index)\n","                # embedding_matrix = pickle_load_embedding_matrix()\n","\n","            embedding_weights = embedding_matrix\n","\n","        sentence_in = Input(shape=(self.MAX_SENTENCE_LENGTH,), dtype='int32', name=\"input_1\")\n","        \n","        embedding_trainable = True\n","      \n","        \n","        if word_embedding_type is 'pre_trained':\n","            embedding_trainable = False\n","        \n","        embedded_word_seq = Embedding(\n","            self.VOCABULARY_SIZE,\n","            embedding_dim,\n","            weights=[embedding_weights],\n","            input_length=self.MAX_SENTENCE_LENGTH,\n","            trainable=embedding_trainable,\n","            #mask_zero=True,\n","            mask_zero=False,\n","            name='word_embeddings',)(sentence_in) \n","        \n","        \n","                     \n","        dropout = Dropout(0.2)(embedded_word_seq)\n","        filter_sizes = [3,4,5]\n","        convs = []\n","        for filter_size in filter_sizes:\n","            conv = Conv1D(filters=64, kernel_size=filter_size, padding='same', activation='relu')(dropout)\n","            pool = MaxPool1D(filter_size)(conv)\n","            convs.append(pool)\n","        \n","        concatenate = Concatenate(axis=1)(convs)\n","        \n","        if rnn_type is 'GRU':\n","            #word_encoder = Bidirectional(CuDNNGRU(50, return_sequences=True, dropout=0.2))(concatenate)                \n","            dropout = Dropout(0.1)(concatenate)\n","            word_encoder = Bidirectional(CuDNNGRU(50, return_sequences=True))(dropout)                \n","        else:\n","            word_encoder = Bidirectional(\n","                LSTM(50, return_sequences=True, dropout=0.2))(embedded_word_seq)\n","            \n","        \n","        dense_transform_word = Dense(\n","            100, \n","            activation='relu', \n","            name='dense_transform_word', \n","            kernel_regularizer=l2_reg)(word_encoder)\n","        \n","        # word attention\n","        attention_weighted_sentence = Model(\n","            sentence_in, Attention(name=\"word_attention\")(dense_transform_word))\n","        \n","        self.word_attention_model = attention_weighted_sentence\n","        \n","        attention_weighted_sentence.summary()\n","\n","        # sentence-attention-weighted document scores\n","        \n","        texts_in = Input(shape=(self.MAX_SENTENCE_COUNT, self.MAX_SENTENCE_LENGTH), dtype='int32', name=\"input_2\")\n","        \n","        attention_weighted_sentences = TimeDistributed(attention_weighted_sentence)(texts_in)\n","        \n","        \n","        if rnn_type is 'GRU':\n","            #sentence_encoder = Bidirectional(GRU(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.2))(attention_weighted_sentences)\n","            dropout = Dropout(0.1)(attention_weighted_sentences)\n","            sentence_encoder = Bidirectional(CuDNNGRU(50, return_sequences=True))(dropout)\n","        else:\n","            sentence_encoder = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.2))(attention_weighted_sentences)\n","        \n","        \n","        dense_transform_sentence = Dense(\n","            100, \n","            activation='relu', \n","            name='dense_transform_sentence',\n","            kernel_regularizer=l2_reg)(sentence_encoder)\n","        \n","        # sentence attention\n","        attention_weighted_text = Attention(name=\"sentence_attention\")(dense_transform_sentence)\n","        \n","        \n","        prediction = Dense(n_classes, activation='softmax')(attention_weighted_text)\n","        \n","        model = Model(texts_in, prediction)\n","        model.summary()\n","        \n","        \n","        optimizer=Adam(lr=learning_rate, decay=0.0001)\n","\n","        model.compile(\n","                      optimizer=optimizer,\n","                      loss='categorical_crossentropy',\n","                      metrics=['accuracy',\n","                               f1,\n","                               tf.keras.metrics.Recall(), \n","                               tf.keras.metrics.Precision()])\n","\n","        return model\n","\n","\n","    def get_tokenizer_filename(self, saved_model_filename):\n","        return saved_model_filename + '.tokenizer'\n","\n","    def fit_on_texts(self, texts):\n","        self.tokenizer = Tokenizer(filters='\"()*,-/;[\\]^_`{|}~', oov_token='UNK');\n","        all_sentences = []\n","        max_sentence_count = 0\n","        max_sentence_length = 0\n","        for text in texts: \n","            sentence_count = len(text)\n","            if sentence_count > max_sentence_count:\n","                max_sentence_count = sentence_count\n","            for sentence in text:\n","                sentence_length = len(sentence)\n","                if sentence_length > max_sentence_length:\n","                    max_sentence_length = sentence_length\n","                all_sentences.append(sentence)\n","\n","\n","        self.MAX_SENTENCE_COUNT = min(max_sentence_count, 15)\n","        self.MAX_SENTENCE_LENGTH = min(max_sentence_length, 50)\n","        \n","        self.tokenizer.fit_on_texts(all_sentences)\n","        self.VOCABULARY_SIZE = len(self.tokenizer.word_index) + 1\n","        self.create_reverse_word_index()\n","\n","    def create_reverse_word_index(self):\n","        self.reverse_word_index = {value:key for key,value in self.tokenizer.word_index.items()}\n","\n","    def encode_texts(self, texts):\n","        encoded_texts = np.zeros((len(texts), self.MAX_SENTENCE_COUNT, self.MAX_SENTENCE_LENGTH))\n","        for i, text in enumerate(texts):  \n","            encoded_text = np.array(pad_sequences(\n","                self.tokenizer.texts_to_sequences(text), \n","                maxlen=self.MAX_SENTENCE_LENGTH))[:self.MAX_SENTENCE_COUNT]\n","            encoded_texts[i][-len(encoded_text):] = encoded_text\n","        return encoded_texts\n","\n","    def save_tokenizer_on_epoch_end(self, path, epoch):\n","        if epoch == 0:\n","            tokenizer_state = {\n","                'tokenizer': self.tokenizer,\n","                'maxSentenceCount': self.MAX_SENTENCE_COUNT,  \n","                'maxSentenceLength': self.MAX_SENTENCE_LENGTH,\n","                'vocabularySize': self.VOCABULARY_SIZE\n","            }\n","            pickle.dump(tokenizer_state, open(path, \"wb\" ) )\n","\n","    def train(self, train_x, train_y, test_x = None, \n","              val_data = None,\n","              batch_size=16, \n","              epochs=1, \n","              embedding_dim=200, \n","              embeddings_path=False, \n","              saved_model_dir= HANN_path + 'saved_models', \n","              saved_model_filename=None,):\n","        \n","        self.fit_on_texts(train_x)\n","\n","        self.model = self.build_model(  \n","            n_classes=train_y.shape[-1], \n","            embedding_dim=200,\n","            embeddings_path=embeddings_path)\n","        \n","        encoded_train_x = self.encode_texts(train_x)\n","\n","        early_stopping = EarlyStopping(monitor='val_f1', mode='max', verbose=1, patience=4)\n","        callbacks = [\n","            ReduceLROnPlateau(),\n","            LambdaCallback(\n","                on_epoch_end=lambda epoch, logs: self.save_tokenizer_on_epoch_end(\n","                    os.path.join(saved_model_dir, \n","                        self.get_tokenizer_filename(saved_model_filename)), epoch)),\n","            early_stopping\n","        ]\n","\n","        \n","\n","        if saved_model_filename:\n","            callbacks.append(\n","                ModelCheckpoint(\n","                    filepath=os.path.join(saved_model_dir, saved_model_filename),\n","                    monitor='val_f1',\n","                    save_best_only=True,\n","                    save_weights_only=False,\n","                )\n","            )\n","        history = self.model.fit(\n","                       x=encoded_train_x, \n","                       y=train_y, \n","                       batch_size=batch_size, \n","                       epochs=epochs, \n","                       verbose=1, \n","                       callbacks=callbacks,\n","                       validation_split=0.1,\n","                      #  validation_data =  val_data,\n","                       shuffle=False)\n","        \n","        # predictions = None\n","        # if(test_x) :\n","        \n","        #   predictions = self.model.predict([test_x], batch_size=batch_size, verbose=1)\n","        \n","        # # Plot\n","        # print(history.history.keys())\n","        \n","        # plt.plot(history.history['accuracy'])\n","        # plt.plot(history.history['val_accuracy'])\n","        # plt.title('model accuracy')\n","        # plt.ylabel('accuracy')\n","        # plt.xlabel('epoch')\n","        # plt.legend(['train', 'test'], loc='upper left')\n","        # plt.show()\n","        \n","        # plt.plot(history.history['loss'])\n","        # plt.plot(history.history['val_loss'])\n","        # plt.title('model loss')\n","        # plt.ylabel('loss')\n","        # plt.xlabel('epoch')\n","        # plt.legend(['train', 'test'], loc='upper left')\n","        # plt.show()\n","\n","        return self.model, history\n","\n","    def encode_input(self, x, log=False):\n","        x = np.array(x)\n","        if not x.shape:\n","            x = np.expand_dims(x, 0)\n","        texts = np.array([normalize(text) for text in x])\n","        return self.encode_texts(texts)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"obo_fTOvUsi8","colab_type":"text"},"source":["# Load dataset\n","\n","This might take several minutes, depending your dataset"]},{"cell_type":"code","metadata":{"id":"jFwuRFg3qxBW","colab_type":"code","trusted":true,"colab":{}},"source":["# YELP_DATA_PATH = HANN_path + 'datasets/yelp_reviews_sampling.json'\n","# IMDB_DATA_PATH = HANN_path + 'datasets/imdb_reviews.csv'\n","# SAVED_MODEL_DIR = HANN_path + 'saved_models'\n","# SAVED_MODEL_FILENAME = 'model_sinhala.h5'\n","\n","# if dataset is 'yelp':\n","#     (X, Y) = load_data_yelp(path=YELP_DATA_PATH, size=400000)\n","# else:\n","#     (X, Y) = load_data_imdb(path=IMDB_DATA_PATH, size=49000)\n","\n","# with open(datasets_path + 'loaded_imdb.pickle', 'wb') as f:\n","#     pickle.dump((X, Y), f)\n","\n","# with open(datasets_path + 'loaded_imdb.pickle', 'rb') as f:\n","#      (X, Y) = pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D9Zcz1wv0wf3","colab_type":"text"},"source":["# Plots Word Embedding chart\n"]},{"cell_type":"code","metadata":{"id":"0TD83uVu04z7","colab_type":"code","trusted":true,"colab":{}},"source":["# limit = 200\n","# vector_dim = 200\n","\n","# # Fasttext\n","# filename = fasttext_path + 'fasttext_model.txt'                \n","# model =  gensim.models.FastText.load(filename)\n","# words = []\n","# embedding = np.array([])\n","# i = 0\n","# for word in model.wv.vocab:\n","#     if i == limit: break\n","\n","#     words.append(word)\n","#     embedding = np.append(embedding, model[word])\n","#     i += 1\n","\n","# embedding = embedding.reshape(limit, vector_dim)    \n","# tsne = TSNE(n_components=2)\n","# low_dim_embedding = tsne.fit_transform(embedding)\n","       \n","# plot_with_labels(low_dim_embedding, words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qOZg-uB4HV1H","colab_type":"text"},"source":["# Taining model"]},{"cell_type":"code","metadata":{"id":"2kV3_6pVATjU","colab_type":"code","colab":{}},"source":["def f1(y_true, y_pred):\n","  def recall(y_true, y_pred):\n","      \"\"\"Recall metric.\n","\n","      Only computes a batch-wise average of recall.\n","\n","      Computes the recall, a metric for multi-label classification of\n","      how many relevant items are selected.\n","      \"\"\"\n","      true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","      possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","      recall = true_positives / (possible_positives + K.epsilon())\n","      return recall\n","\n","  def precision(y_true, y_pred):\n","      \"\"\"Precision metric.\n","\n","      Only computes a batch-wise average of precision.\n","\n","      Computes the precision, a metric for multi-label classification of\n","      how many selected items are relevant.\n","      \"\"\"\n","      true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","      predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","      precision = true_positives / (predicted_positives + K.epsilon())\n","      return precision\n","  precision = precision(y_true, y_pred)\n","  recall = recall(y_true, y_pred)\n","  return 2*((precision*recall)/(precision+recall+K.epsilon()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ZkHlV6rHNkJ","colab_type":"code","trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597998278499,"user_tz":-330,"elapsed":300355,"user":{"displayName":"dataPirates fyp","photoUrl":"","userId":"14020399926305222994"}},"outputId":"e10418c7-b0b5-44a0-c4aa-5252db042224"},"source":["K.clear_session()\n","model = HAHNetwork()\n","model.train(X, Y, batch_size=64, epochs=20, embeddings_path=True, saved_model_dir=SAVED_MODEL_DIR, saved_model_filename=SAVED_MODEL_FILENAME)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"},{"output_type":"stream","text":["tracking <tf.Variable 'word_attention/Variable:0' shape=(100, 1) dtype=float32> W\n","tracking <tf.Variable 'word_attention/Variable_1:0' shape=(50,) dtype=float32> b\n","tracking <tf.Variable 'word_attention/Variable_2:0' shape=(50, 1) dtype=float32> u\n","Model: \"model_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, 50)                0         \n","_________________________________________________________________\n","word_embeddings (Embedding)  (None, 50, 300)           15749100  \n","_________________________________________________________________\n","bidirectional_1 (Bidirection (None, 50, 100)           140400    \n","_________________________________________________________________\n","dense_transform_word (Dense) (None, 50, 100)           10100     \n","_________________________________________________________________\n","word_attention (Attention)   (None, 100)               200       \n","=================================================================\n","Total params: 15,899,800\n","Trainable params: 150,700\n","Non-trainable params: 15,749,100\n","_________________________________________________________________\n","tracking <tf.Variable 'sentence_attention/Variable:0' shape=(100, 1) dtype=float32> W\n","tracking <tf.Variable 'sentence_attention/Variable_1:0' shape=(50,) dtype=float32> b\n","tracking <tf.Variable 'sentence_attention/Variable_2:0' shape=(50, 1) dtype=float32> u\n","Model: \"model_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_2 (InputLayer)         (None, 15, 50)            0         \n","_________________________________________________________________\n","time_distributed_1 (TimeDist (None, 15, 100)           15899800  \n","_________________________________________________________________\n","bidirectional_2 (Bidirection (None, 15, 100)           60400     \n","_________________________________________________________________\n","dense_transform_sentence (De (None, 15, 100)           10100     \n","_________________________________________________________________\n","sentence_attention (Attentio (None, 100)               200       \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 4)                 404       \n","=================================================================\n","Total params: 15,970,904\n","Trainable params: 221,804\n","Non-trainable params: 15,749,100\n","_________________________________________________________________\n","Train on 13553 samples, validate on 1506 samples\n","Epoch 1/20\n","13553/13553 [==============================] - 257s 19ms/step - loss: 1.2644 - accuracy: 0.4789 - f1: 0.2410 - recall: 0.0625 - precision: 0.3970 - val_loss: 1.0111 - val_accuracy: 0.6016 - val_f1: 0.5843 - val_recall: 0.2342 - val_precision: 0.6610\n","Epoch 2/20\n","13553/13553 [==============================] - 253s 19ms/step - loss: 1.0790 - accuracy: 0.5878 - f1: 0.4829 - recall: 0.2698 - precision: 0.6614 - val_loss: 0.9235 - val_accuracy: 0.6554 - val_f1: 0.6018 - val_recall: 0.3370 - val_precision: 0.6774\n","Epoch 3/20\n","13553/13553 [==============================] - 256s 19ms/step - loss: 0.9946 - accuracy: 0.6275 - f1: 0.5839 - recall: 0.3671 - precision: 0.6794 - val_loss: 0.8774 - val_accuracy: 0.6746 - val_f1: 0.6366 - val_recall: 0.3991 - val_precision: 0.6878\n","Epoch 4/20\n","13553/13553 [==============================] - 257s 19ms/step - loss: 0.9608 - accuracy: 0.6376 - f1: 0.6079 - recall: 0.4165 - precision: 0.6924 - val_loss: 0.8593 - val_accuracy: 0.6766 - val_f1: 0.6536 - val_recall: 0.4348 - val_precision: 0.6986\n","Epoch 5/20\n","13553/13553 [==============================] - 257s 19ms/step - loss: 0.9405 - accuracy: 0.6376 - f1: 0.6102 - recall: 0.4448 - precision: 0.7015 - val_loss: 0.8469 - val_accuracy: 0.6833 - val_f1: 0.6486 - val_recall: 0.4562 - val_precision: 0.7064\n","Epoch 6/20\n","13553/13553 [==============================] - 259s 19ms/step - loss: 0.9175 - accuracy: 0.6469 - f1: 0.6189 - recall: 0.4632 - precision: 0.7087 - val_loss: 0.8383 - val_accuracy: 0.6846 - val_f1: 0.6502 - val_recall: 0.4716 - val_precision: 0.7124\n","Epoch 7/20\n","13553/13553 [==============================] - 263s 19ms/step - loss: 0.9039 - accuracy: 0.6519 - f1: 0.6168 - recall: 0.4761 - precision: 0.7142 - val_loss: 0.8325 - val_accuracy: 0.6873 - val_f1: 0.6511 - val_recall: 0.4822 - val_precision: 0.7168\n","Epoch 8/20\n","13553/13553 [==============================] - 263s 19ms/step - loss: 0.8883 - accuracy: 0.6542 - f1: 0.6274 - recall: 0.4861 - precision: 0.7182 - val_loss: 0.8308 - val_accuracy: 0.6740 - val_f1: 0.6538 - val_recall: 0.4915 - val_precision: 0.7205\n","Epoch 9/20\n","13553/13553 [==============================] - 265s 20ms/step - loss: 0.8737 - accuracy: 0.6585 - f1: 0.6299 - recall: 0.4943 - precision: 0.7218 - val_loss: 0.8233 - val_accuracy: 0.6813 - val_f1: 0.6529 - val_recall: 0.4988 - val_precision: 0.7240\n","Epoch 10/20\n","13553/13553 [==============================] - 266s 20ms/step - loss: 0.8607 - accuracy: 0.6601 - f1: 0.6324 - recall: 0.5012 - precision: 0.7251 - val_loss: 0.8198 - val_accuracy: 0.6826 - val_f1: 0.6549 - val_recall: 0.5048 - val_precision: 0.7272\n","Epoch 11/20\n","13553/13553 [==============================] - 257s 19ms/step - loss: 0.8394 - accuracy: 0.6680 - f1: 0.6433 - recall: 0.5073 - precision: 0.7282 - val_loss: 0.8165 - val_accuracy: 0.6813 - val_f1: 0.6558 - val_recall: 0.5108 - val_precision: 0.7301\n","Epoch 12/20\n","13553/13553 [==============================] - 261s 19ms/step - loss: 0.8234 - accuracy: 0.6706 - f1: 0.6450 - recall: 0.5125 - precision: 0.7312 - val_loss: 0.8152 - val_accuracy: 0.6793 - val_f1: 0.6490 - val_recall: 0.5156 - val_precision: 0.7329\n","Epoch 13/20\n","13553/13553 [==============================] - 266s 20ms/step - loss: 0.8054 - accuracy: 0.6746 - f1: 0.6523 - recall: 0.5173 - precision: 0.7342 - val_loss: 0.8286 - val_accuracy: 0.6760 - val_f1: 0.6470 - val_recall: 0.5202 - val_precision: 0.7359\n","Epoch 14/20\n","13553/13553 [==============================] - 268s 20ms/step - loss: 0.7902 - accuracy: 0.6793 - f1: 0.6559 - recall: 0.5216 - precision: 0.7371 - val_loss: 0.8368 - val_accuracy: 0.6746 - val_f1: 0.6500 - val_recall: 0.5242 - val_precision: 0.7387\n","Epoch 15/20\n","13553/13553 [==============================] - 269s 20ms/step - loss: 0.7746 - accuracy: 0.6886 - f1: 0.6593 - recall: 0.5255 - precision: 0.7399 - val_loss: 0.8377 - val_accuracy: 0.6753 - val_f1: 0.6558 - val_recall: 0.5279 - val_precision: 0.7414\n","Epoch 00015: early stopping\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(<keras.engine.training.Model at 0x7f9abe9c7a90>,\n"," <keras.callbacks.callbacks.History at 0x7f9abd395a90>,\n"," None)"]},"metadata":{"tags":[]},"execution_count":113}]},{"cell_type":"markdown","metadata":{"id":"y42j8o52hSBA","colab_type":"text"},"source":["# Get Predictions"]},{"cell_type":"code","metadata":{"id":"w_GN-UXPiR4P","colab_type":"code","colab":{}},"source":["X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7NOO6JhsiS7N","colab_type":"code","colab":{}},"source":["K.clear_session()\n","model = HAHNetwork()\n","model, history, predictions = model.train(X_train[:100], y_train[:100], test_x=X_test, batch_size=64, epochs=1, embeddings_path=True, saved_model_dir=SAVED_MODEL_DIR, saved_model_filename=SAVED_MODEL_FILENAME)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nxZO2_eHhRe1","colab_type":"code","colab":{}},"source":["model_save_path = os.path.join(SAVED_MODEL_DIR, SAVED_MODEL_FILENAME)\n","\n","loaded_model  = load_model(model_save_path,custom_objects={\"f1\": f1}, compile=True)\n","print(\"loaded \" + loaded_model.name)\n","\n","predictions = loaded_model.predict(X_test, batch_size=BATCH_SIZE, verbose=0)\n","\n","labels = np.argmax(y_test, axis=1)\n","predictions = np.argmax(predictions, axis=1)\n","\n","report = classification_report(labels, predictions, digits=4,output_dict=True)\n","report_print = classification_report(labels, predictions, digits=4)\n","print(report_print)\n","report_df = pd.DataFrame(report).transpose()\n","\n","report_name = os.path.join(SAVED_MODEL_DIR, \"lstm_with_puncuationmarks.csv\")\n","\n","report_df.to_csv(report_name)"],"execution_count":null,"outputs":[]}]}