{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sinhala_text_capsules.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "environment": {
      "name": "tf-gpu.1-15.m50",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m50"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8D9EuJnBnDV"
      },
      "source": [
        "## **Installing and importing dependecies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgRvEbMXy9nc",
        "outputId": "233f3a89-5fcb-4026-87bb-18c8bcf94a48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RJTK5LXWz6Z",
        "outputId": "7ba5dfe7-a652-482a-800f-e3d449ecc7c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install tensorflow==1.14.0 "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.14.0 in /usr/local/lib/python3.6/dist-packages (1.14.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.18.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.33.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (3.12.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.14.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.2)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.14.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.3.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.35.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.14.0) (50.3.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOF29ZnzBjht"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlXBE_-86l4q",
        "outputId": "3c6fb883-5f4e-4b7a-84f8-91003ad1d8ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import collections\n",
        "import gensim\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from gensim.models.fasttext import FastText\n",
        "from gensim.models import word2vec\n",
        "\n",
        "from sklearn.model_selection import train_test_split,cross_val_score, cross_val_predict, KFold, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "import matplotlib.image  as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.layers.python.layers import initializers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb6SNszhX4Jc",
        "outputId": "a0103aa0-1e1f-48ab-d57d-281c930d477e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install keras==2.1.5"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras==2.1.5 in /usr/local/lib/python3.6/dist-packages (2.1.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.1.5) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.5) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.5) (1.18.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.5) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QeMxau54aMg"
      },
      "source": [
        "#set folder paths\n",
        "folder_path = ''\n",
        "EMBEDDING_SIZE = 300 \n",
        "embedding_type = \"fasttext\"\n",
        "context = 5\n",
        "\n",
        "\n",
        "lankadeepa_data_path = folder_path + 'corpus/new/preprocess_from_isuru/lankadeepa_tagged_comments.csv'\n",
        "gossip_lanka_data_path = folder_path + 'corpus/new/preprocess_from_unicode_values/gossip_lanka_tagged_comments.csv'\n",
        "\n",
        "\n",
        "word_embedding_path = folder_path + \"word_embedding/\"+embedding_type+\"/source2_data_from_gosspiLanka_and_lankadeepa/\"+str(EMBEDDING_SIZE)+\"/\"+embedding_type+\"_\"+str(EMBEDDING_SIZE)+\"_\"+str(context)\n",
        "word_embedding_keydvectors_path = folder_path + \"word_embedding/\"+embedding_type+\"/source2_data_from_gosspiLanka_and_lankadeepa/\"+str(EMBEDDING_SIZE)+\"/keyed_vectors/keyed.kv\"\n",
        "embedding_matrix_path = folder_path + 'Sentiment Analysis/CNN RNN/embedding_matrix/'+embedding_type+'_lankadeepa_gossiplanka_'+str(EMBEDDING_SIZE)+'_'+str(context)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRbpY2FTDl8p"
      },
      "source": [
        "## **Load Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyuNHf_14aTQ"
      },
      "source": [
        "lankadeepa_data = pd.read_csv(lankadeepa_data_path)[:9059]\n",
        "gossipLanka_data = pd.read_csv(gossip_lanka_data_path)\n",
        "gossipLanka_data = gossipLanka_data.drop(columns=['Unnamed: 3'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QEa9WKy4aao",
        "outputId": "6cf91487-e505-41db-ae33-64e21d70da7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "lankadeepa_data"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>docid</th>\n",
              "      <th>comment</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100204</td>\n",
              "      <td>නියම සිංහල මහත්මයෙක්.</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100204</td>\n",
              "      <td>අන්න මිනිස්සු...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100204</td>\n",
              "      <td>හොරා හොද මිනිහෙක් නම් අනේ මහත්තයෝ සමාවෙන්න කිය...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100204</td>\n",
              "      <td>තව ඩිංගෙන් ලබු ගෙඩියත් කොස් ගෙඩි සහ පොල් ගෙඩි ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100204</td>\n",
              "      <td>බතල වැලට කොස්ගෙඩි අටට උසාවි ගිහින් රිමන්ඩ් කරන...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9053</th>\n",
              "      <td>460747</td>\n",
              "      <td>එ්ක තමයි මෙයාලා කෑගහන්නේ මේ රජය අසමත් කිය කියා.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9054</th>\n",
              "      <td>460747</td>\n",
              "      <td>මේගොල්ලෝ කිසි දවසක හිරේ යන්නේ නැහැ. එ්ක ස්ථීරය...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9055</th>\n",
              "      <td>460747</td>\n",
              "      <td>දූෂණ වංචා සම්බන්ධයෙන් චෝදනා ලැබූ පසුගිය ආණ්‌ඩු...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9056</th>\n",
              "      <td>460747</td>\n",
              "      <td>වැරදිකාරයන්ට දඬුවම් කරන්න ඕනෑ</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9057</th>\n",
              "      <td>460747</td>\n",
              "      <td>රාජපක්ෂවරුන් අැමෙරිකාවෙි පුරවැසියන් ඹවුන්ට මෙහ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9058 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       docid                                            comment  label\n",
              "0     100204                              නියම සිංහල මහත්මයෙක්.      4\n",
              "1     100204                                   අන්න මිනිස්සු...      4\n",
              "2     100204  හොරා හොද මිනිහෙක් නම් අනේ මහත්තයෝ සමාවෙන්න කිය...      3\n",
              "3     100204  තව ඩිංගෙන් ලබු ගෙඩියත් කොස් ගෙඩි සහ පොල් ගෙඩි ...      2\n",
              "4     100204  බතල වැලට කොස්ගෙඩි අටට උසාවි ගිහින් රිමන්ඩ් කරන...      2\n",
              "...      ...                                                ...    ...\n",
              "9053  460747    එ්ක තමයි මෙයාලා කෑගහන්නේ මේ රජය අසමත් කිය කියා.      2\n",
              "9054  460747  මේගොල්ලෝ කිසි දවසක හිරේ යන්නේ නැහැ. එ්ක ස්ථීරය...      2\n",
              "9055  460747  දූෂණ වංචා සම්බන්ධයෙන් චෝදනා ලැබූ පසුගිය ආණ්‌ඩු...      2\n",
              "9056  460747                      වැරදිකාරයන්ට දඬුවම් කරන්න ඕනෑ      2\n",
              "9057  460747  රාජපක්ෂවරුන් අැමෙරිකාවෙි පුරවැසියන් ඹවුන්ට මෙහ...      2\n",
              "\n",
              "[9058 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r88f-JQ4aIq",
        "outputId": "c585bcda-b108-4332-a801-dc23b3348cbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "all_data = pd.concat([lankadeepa_data,gossipLanka_data], ignore_index=True)\n",
        "all_data.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15059, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dh6y6RxDv6w"
      },
      "source": [
        "## **Process data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NJfQGfj7VMz"
      },
      "source": [
        "def text_preprocessing_2(data):\n",
        "  comments = data['comment']\n",
        "  labels = data['label']\n",
        "  comments_splitted = []\n",
        "\n",
        "  for comment in comments:\n",
        "    lines = []\n",
        "    try:\n",
        "      words = comment.split()\n",
        "      lines += words\n",
        "    except:\n",
        "      continue\n",
        "    comments_splitted.append(lines)\n",
        "\n",
        "  return comments_splitted,labels"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiasDeTS66ef",
        "outputId": "dd1f9cf8-b441-4f71-b3aa-3a2a96c341fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "comment_texts, comment_labels = text_preprocessing_2(all_data)\n",
        "\n",
        "# prepare tokenizer\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(comment_texts)\n",
        "vocab_size = len(t.word_index) + 1\n",
        "print(vocab_size)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3KdBCcN66aU"
      },
      "source": [
        "encoded_docs = t.texts_to_sequences(comment_texts)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ1R0ipc66XX"
      },
      "source": [
        "max_length = len(max(encoded_docs, key=len))\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length,padding='post')\n",
        "comment_labels = np.array(comment_labels)\n",
        "padded_docs = np.array(padded_docs)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtFzHWCC66U5"
      },
      "source": [
        "comment_labels = pd.get_dummies(comment_labels).values"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL3qusEd70hr"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(padded_docs, comment_labels, test_size=0.1, random_state=0)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1iFKGQvgGNm"
      },
      "source": [
        "# **Load Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd3DeM14Rpp9"
      },
      "source": [
        "#set embedding path to load embeddings\n",
        "embedding_path = ''"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2AYK7My_wEK"
      },
      "source": [
        "def generate_embedding_matrix():\n",
        "  \n",
        "  word_embedding_model = FastText.load(embedding_path)\n",
        "\n",
        "    \n",
        "  word_vectors = word_embedding_model.wv\n",
        "  word_vectors.save(word_embedding_keydvectors_path)\n",
        "  word_vectors = KeyedVectors.load(word_embedding_keydvectors_path, mmap='r')\n",
        "\n",
        "  embeddings_index = dict()\n",
        "  for word, vocab_obj in word_vectors.vocab.items():\n",
        "    embeddings_index[word]=word_vectors[word]\n",
        "\n",
        "  # create a weight matrix for words in training docs\n",
        "  embedding_matrix = np.zeros((vocab_size, EMBEDDING_SIZE))\n",
        "  for word, i in t.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  # pickle.dump(embedding_matrix, open(embedding_matrix_path, 'wb'))\n",
        "  return embedding_matrix"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjJm-nkjRpm5"
      },
      "source": [
        "embedding_vectors = generate_embedding_matrix()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk7iSweShnqA"
      },
      "source": [
        "maxlen = max_length\n",
        "max_features = vocab_size\n",
        "embedding_dims = 300"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UehnBw_8exkL"
      },
      "source": [
        "# **Capsule Network Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z06E9a1aBVb"
      },
      "source": [
        "def _conv2d_wrapper(inputs, shape, strides, padding, add_bias, activation_fn, name, stddev=0.1):\n",
        "  with tf.variable_scope(name,reuse=tf.AUTO_REUSE) as scope:\n",
        "    kernel = _get_weights_wrapper(\n",
        "      name='weights', shape=shape, weights_decay_factor=0.0, )\n",
        "    output = tf.nn.conv2d(inputs, filter=kernel, strides=strides, padding=padding, name='conv')\n",
        "    if add_bias:\n",
        "      biases = _get_biases_wrapper(name='biases', shape=[shape[-1]] )\n",
        "      output = tf.add(output, biases, name='biasAdd')\n",
        "    if activation_fn is not None:\n",
        "      output = activation_fn(output, name='activation')\n",
        "  return output"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8F7Y8CGbsB4"
      },
      "source": [
        "def _get_weights_wrapper(name, shape, dtype=tf.float32, initializer=initializers.xavier_initializer(),weights_decay_factor=None):\n",
        "  weights = _get_variable_wrapper(name=name, shape=shape, dtype=dtype, initializer=initializer)\n",
        "  if weights_decay_factor is not None and weights_decay_factor > 0.0:\n",
        "    weights_wd = tf.multiply(tf.nn.l2_loss(weights), weights_decay_factor, name=name + '/l2loss')\n",
        "    tf.add_to_collection('losses', weights_wd)\n",
        "  return weights"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkO3w5gDcC1b"
      },
      "source": [
        "def _get_biases_wrapper(name, shape, dtype=tf.float32, initializer=tf.constant_initializer(0.0)):\n",
        "  \"\"\"Wrapper over _get_variable_wrapper() to get bias.\n",
        "  \"\"\"\n",
        "  biases = _get_variable_wrapper(name=name, shape=shape, dtype=dtype, initializer=initializer)\n",
        "  return biases"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GVuHgh2EWw2"
      },
      "source": [
        "def _get_variable_wrapper(name, shape=None, dtype=None, initializer=None,regularizer=None,trainable=True,collections=None,caching_device=None,partitioner=None,validate_shape=True,custom_getter=None):\n",
        "  with tf.device('/cpu:0'):\n",
        "    var = tf.get_variable(\n",
        "      name, shape=shape, dtype=dtype, initializer=initializer,\n",
        "      regularizer=regularizer, trainable=trainable,\n",
        "      collections=collections, caching_device=caching_device,\n",
        "      partitioner=partitioner, validate_shape=validate_shape,\n",
        "      custom_getter=custom_getter\n",
        "    )\n",
        "  return var"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBkd3MRuccQh"
      },
      "source": [
        "def softmax(x, axis=-1):\n",
        "    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
        "    return ex/K.sum(ex, axis=axis, keepdims=True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSVP0ymhc6G6"
      },
      "source": [
        "def squash_v1(x, axis=-1):\n",
        "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
        "    scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
        "    return scale * x"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQzm6uHwc8Rq"
      },
      "source": [
        "def squash_v0(s, axis=-1, epsilon=1e-7, name=None):\n",
        "    s_squared_norm = K.sum(K.square(s), axis, keepdims=True) + K.epsilon()\n",
        "    safe_norm = K.sqrt(s_squared_norm)\n",
        "    scale = 1 - tf.exp(-safe_norm)\n",
        "    return scale * s / safe_norm"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObeP2wKDc-Zq"
      },
      "source": [
        "def routing(u_hat_vecs, beta_a, iterations, output_capsule_num, i_activations):\n",
        "    b = keras.backend.zeros_like(u_hat_vecs[:,:,:,0])\n",
        "    if i_activations is not None:\n",
        "        i_activations = i_activations[...,tf.newaxis]\n",
        "    for i in range(iterations):\n",
        "        if False:\n",
        "            leak = tf.zeros_like(b, optimize=True)\n",
        "            leak = tf.reduce_sum(leak, axis=1, keep_dims=True)\n",
        "            leaky_logits = tf.concat([leak, b], axis=1)\n",
        "            leaky_routing = tf.nn.softmax(leaky_logits, dim=1)        \n",
        "            c = tf.split(leaky_routing, [1, output_capsule_num], axis=1)[1]\n",
        "        else:\n",
        "            c = softmax(b, 1)   \n",
        "        outputs = squash_v1(K.batch_dot(c, u_hat_vecs, [2, 2]))\n",
        "        if i < iterations - 1:\n",
        "            b = b + K.batch_dot(outputs, u_hat_vecs, [2, 3])                                    \n",
        "    poses = outputs \n",
        "    activations = K.sqrt(K.sum(K.square(poses), 2))\n",
        "    return poses, activations\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSdVM9YCdEMp"
      },
      "source": [
        "def vec_transformationByConv(poses, input_capsule_dim, input_capsule_num, output_capsule_dim, output_capsule_num):                            \n",
        "    kernel = _get_weights_wrapper(name='weights', shape=[1, input_capsule_dim, output_capsule_dim*output_capsule_num], weights_decay_factor=0.0)\n",
        "    u_hat_vecs = keras.backend.conv1d(poses, kernel)\n",
        "    u_hat_vecs = keras.backend.reshape(u_hat_vecs, (-1, input_capsule_num, output_capsule_num, output_capsule_dim))\n",
        "    u_hat_vecs = keras.backend.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
        "    return u_hat_vecs\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-pIavredIK6"
      },
      "source": [
        "def vec_transformationByMat(poses, input_capsule_dim, input_capsule_num, output_capsule_dim, output_capsule_num, shared=True):                        \n",
        "    inputs_poses_shape = poses.get_shape().as_list()\n",
        "    poses = poses[..., tf.newaxis, :]        \n",
        "    poses = tf.tile(poses, [1, 1, output_capsule_num, 1])    \n",
        "    if shared:\n",
        "        kernel = _get_weights_wrapper(name='weights', shape=[1, 1, output_capsule_num, output_capsule_dim, input_capsule_dim], weights_decay_factor=0.0)\n",
        "        kernel = tf.tile(kernel, [inputs_poses_shape[0], input_capsule_num, 1, 1, 1])\n",
        "    else:\n",
        "        kernel = _get_weights_wrapper(name='weights', shape=[1, input_capsule_num, output_capsule_num, output_capsule_dim, input_capsule_dim], weights_decay_factor=0.0)\n",
        "        kernel = tf.tile(kernel, [inputs_poses_shape[0], 1, 1, 1, 1])\n",
        "    u_hat_vecs = tf.squeeze(tf.matmul(kernel, poses[...,tf.newaxis]),axis=-1)\n",
        "    u_hat_vecs = keras.backend.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
        "    return u_hat_vecs"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcvHPFCOdKyX"
      },
      "source": [
        "def capsules_init(inputs, shape, strides, padding, pose_shape, add_bias, name):\n",
        "    with tf.variable_scope(name,reuse=tf.AUTO_REUSE):   \n",
        "        poses = _conv2d_wrapper(\n",
        "          inputs,\n",
        "          shape=shape[0:-1] + [shape[-1] * pose_shape],\n",
        "          strides=strides,\n",
        "          padding=padding,\n",
        "          add_bias=add_bias,\n",
        "          activation_fn=None,\n",
        "          name='pose_stacked'\n",
        "        )        \n",
        "        poses_shape = poses.get_shape().as_list()    \n",
        "        poses = tf.reshape(poses, [-1, poses_shape[1], poses_shape[2], shape[-1], pose_shape])        \n",
        "        beta_a = _get_weights_wrapper(name='beta_a', shape=[1, shape[-1]])    \n",
        "        poses = squash_v1(poses, axis=-1)  \n",
        "        activations = K.sqrt(K.sum(K.square(poses), axis=-1)) + beta_a        \n",
        "\n",
        "    return poses, activations"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAJ1njh6dOpT"
      },
      "source": [
        "def capsule_fc_layer(nets, output_capsule_num, iterations, name):\n",
        "    with tf.variable_scope(name,reuse=tf.AUTO_REUSE):   \n",
        "        poses, i_activations = nets\n",
        "        input_pose_shape = poses.get_shape().as_list()\n",
        "\n",
        "        u_hat_vecs = vec_transformationByConv(poses,input_pose_shape[-1], input_pose_shape[1],input_pose_shape[-1], output_capsule_num,)\n",
        "        beta_a = _get_weights_wrapper(name='beta_a', shape=[1, output_capsule_num])\n",
        "        poses, activations = routing(u_hat_vecs, beta_a, iterations, output_capsule_num, i_activations)\n",
        "    return poses, activations"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jE-SPLBwdR2a"
      },
      "source": [
        "def capsule_flatten(nets):\n",
        "    poses, activations = nets\n",
        "    input_pose_shape = poses.get_shape().as_list()\n",
        "    \n",
        "    poses = tf.reshape(poses, [\n",
        "                    -1, input_pose_shape[1]*input_pose_shape[2]*input_pose_shape[3], input_pose_shape[-1]]) \n",
        "    activations = tf.reshape(activations, [\n",
        "                    -1, input_pose_shape[1]*input_pose_shape[2]*input_pose_shape[3]])\n",
        "    return poses, activations"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uMUkhf1dVJy"
      },
      "source": [
        "def capsule_conv_layer(nets, shape, strides, iterations, name):   \n",
        "    with tf.variable_scope(name,reuse=tf.AUTO_REUSE):              \n",
        "        poses, i_activations = nets\n",
        "        \n",
        "        inputs_poses_shape = poses.get_shape().as_list()\n",
        "\n",
        "        hk_offsets = [\n",
        "          [(h_offset + k_offset) for k_offset in range(0, shape[0])] for h_offset in\n",
        "          range(0, inputs_poses_shape[1] + 1 - shape[0], strides[1])\n",
        "        ]\n",
        "        wk_offsets = [\n",
        "          [(w_offset + k_offset) for k_offset in range(0, shape[1])] for w_offset in\n",
        "          range(0, inputs_poses_shape[2] + 1 - shape[1], strides[2])\n",
        "        ]\n",
        "    \n",
        "        inputs_poses_patches = tf.transpose(\n",
        "          tf.gather(\n",
        "            tf.gather(\n",
        "              poses, hk_offsets, axis=1, name='gather_poses_height_kernel'\n",
        "            ), wk_offsets, axis=3, name='gather_poses_width_kernel'\n",
        "          ), perm=[0, 1, 3, 2, 4, 5, 6], name='inputs_poses_patches'\n",
        "        )\n",
        "        inputs_poses_shape = inputs_poses_patches.get_shape().as_list()\n",
        "        inputs_poses_patches = tf.reshape(inputs_poses_patches, [\n",
        "                                -1, shape[0]*shape[1]*shape[2], inputs_poses_shape[-1]\n",
        "                                ])\n",
        "\n",
        "        i_activations_patches = tf.transpose(\n",
        "          tf.gather(\n",
        "            tf.gather(\n",
        "              i_activations, hk_offsets, axis=1, name='gather_activations_height_kernel'\n",
        "            ), wk_offsets, axis=3, name='gather_activations_width_kernel'\n",
        "          ), perm=[0, 1, 3, 2, 4, 5], name='inputs_activations_patches'\n",
        "        )\n",
        "        i_activations_patches = tf.reshape(i_activations_patches, [\n",
        "                                -1, shape[0]*shape[1]*shape[2]]\n",
        "                                )\n",
        "        u_hat_vecs = vec_transformationByConv(\n",
        "                  inputs_poses_patches,\n",
        "                  inputs_poses_shape[-1], shape[0]*shape[1]*shape[2],\n",
        "                  inputs_poses_shape[-1], shape[3],\n",
        "                  )  \n",
        "        beta_a = _get_weights_wrapper(\n",
        "                name='beta_a', shape=[1, shape[3]]\n",
        "                )\n",
        "        poses, activations = routing(u_hat_vecs, beta_a, iterations, shape[3], i_activations_patches)\n",
        "        poses = tf.reshape(poses, [\n",
        "                    inputs_poses_shape[0], inputs_poses_shape[1],\n",
        "                    inputs_poses_shape[2], shape[3],\n",
        "                    inputs_poses_shape[-1]]\n",
        "                ) \n",
        "        activations = tf.reshape(activations, [\n",
        "                    inputs_poses_shape[0],inputs_poses_shape[1],\n",
        "                    inputs_poses_shape[2],shape[3]]\n",
        "                ) \n",
        "        nets = poses, activations            \n",
        "    return nets"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jam_fKSJjYfg"
      },
      "source": [
        "def capsule_model_A(X, num_classes):\n",
        "    with tf.variable_scope('capsule_'+str(3),reuse=tf.AUTO_REUSE ):   \n",
        "        nets = _conv2d_wrapper(\n",
        "                X, shape=[3, 300, 1, 32], strides=[1, 2, 1, 1], padding='VALID', \n",
        "                add_bias=True, activation_fn=tf.nn.relu, name='conv1'\n",
        "            )\n",
        "        nets = capsules_init(nets, shape=[1, 1, 32, 16], strides=[1, 1, 1, 1], \n",
        "                             padding='VALID', pose_shape=16, add_bias=True, name='primary')                        \n",
        "        nets = capsule_conv_layer(nets, shape=[3, 1, 16, 16], strides=[1, 1, 1, 1], iterations=3, name='conv2')\n",
        "        nets = capsule_flatten(nets)\n",
        "        poses, activations = capsule_fc_layer(nets, num_classes, 3, 'fc2') \n",
        "    return poses, activations"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbqUEzcbotXD"
      },
      "source": [
        "def capsule_model_B(X, num_classes):\n",
        "    poses_list = []\n",
        "    for _, ngram in enumerate([3,4,5]):\n",
        "        with tf.variable_scope('capsule_'+str(ngram),reuse=tf.AUTO_REUSE): \n",
        "            nets = _conv2d_wrapper(\n",
        "                X, shape=[ngram, 300, 1, 32], strides=[1, 2, 1, 1], padding='VALID', \n",
        "                add_bias=True, activation_fn=tf.nn.relu, name='conv1'\n",
        "            )\n",
        "            nets = capsules_init(nets, shape=[1, 1, 32, 16], strides=[1, 1, 1, 1], \n",
        "                                 padding='VALID', pose_shape=16, add_bias=True, name='primary')                        \n",
        "            nets = capsule_conv_layer(nets, shape=[3, 1, 16, 16], strides=[1, 1, 1, 1], iterations=3, name='conv2')\n",
        "            nets = capsule_flatten(nets)\n",
        "            poses, activations = capsule_fc_layer(nets, num_classes, 3, 'fc2')\n",
        "            poses_list.append(poses)\n",
        "    \n",
        "    poses = tf.reduce_mean(tf.convert_to_tensor(poses_list), axis=0) \n",
        "    activations = K.sqrt(K.sum(K.square(poses), 2))\n",
        "    return poses, activations"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PARyB28Vi9VI"
      },
      "source": [
        "# **Loss functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkAyzy_3hrYf"
      },
      "source": [
        "def spread_loss(labels, activations, margin):\n",
        "    activations_shape = activations.get_shape().as_list()\n",
        "    mask_t = tf.equal(labels, 1)\n",
        "    mask_i = tf.equal(labels, 0)    \n",
        "    activations_t = tf.reshape(\n",
        "      tf.boolean_mask(activations, mask_t), [activations_shape[0], 1]\n",
        "    )    \n",
        "    activations_i = tf.reshape(\n",
        "      tf.boolean_mask(activations, mask_i), [activations_shape[0], activations_shape[1] - 1]\n",
        "    )    \n",
        "    gap_mit = tf.reduce_sum(tf.square(tf.nn.relu(margin - (activations_t - activations_i))))\n",
        "    return gap_mit        "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2UfkOrSjFvX"
      },
      "source": [
        "def cross_entropy(y, preds):    \n",
        "    y = tf.argmax(y, axis=1)\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=preds, labels=y)                                               \n",
        "    loss = tf.reduce_mean(loss) \n",
        "    return loss"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEI1tgTnjUg_"
      },
      "source": [
        "def margin_loss(y, preds):    \n",
        "    y = tf.cast(y,tf.float32)\n",
        "    loss = y * tf.square(tf.maximum(0., 0.9 - preds)) + \\\n",
        "        0.25 * (1.0 - y) * tf.square(tf.maximum(0., preds - 0.1))\n",
        "    loss = tf.reduce_mean(tf.reduce_sum(loss, axis=1))\n",
        "    return loss"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz6aicUlku28"
      },
      "source": [
        "# **Training process**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un6p6OLXkPnt"
      },
      "source": [
        "class Args:\n",
        "  embedding_type = \"static\"\n",
        "  dataset = \"\"\n",
        "  loss_type = \"margin_loss\"\n",
        "  model_type = \"capsule-B\"\n",
        "  has_test = 1\n",
        "  has_dev = 1\n",
        "  num_epochs = 10\n",
        "  batch_size = 64\n",
        "  use_orphan = False\n",
        "  use_leaky = False\n",
        "  learning_rate = 0.001\n",
        "  margin = 0.2\n",
        "  num_classes = 4\n",
        "  vocab_size = vocab_size\n",
        "  vec_size = 300\n",
        "  max_sent = max_length\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cA5u4HrkuCb"
      },
      "source": [
        "args = Args()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVRu8KNo3GcR"
      },
      "source": [
        "with tf.device('/cpu:0'):\n",
        "    global_step = tf.train.get_or_create_global_step()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFGsOcj50Eov"
      },
      "source": [
        "class BatchGenerator(object):\n",
        "    \"\"\"Generate and hold batches.\"\"\"\n",
        "    def __init__(self, dataset,label, batch_size,input_size, is_shuffle=True):\n",
        "      self._dataset = dataset\n",
        "      self._label = label\n",
        "      self._batch_size = batch_size    \n",
        "      self._cursor = 0      \n",
        "      self._input_size = input_size      \n",
        "      \n",
        "      if is_shuffle:\n",
        "          index = np.arange(len(self._dataset))\n",
        "          np.random.shuffle(index)\n",
        "          self._dataset = np.array(self._dataset)[index]\n",
        "          self._label = np.array(self._label)[index]\n",
        "      else:\n",
        "          self._dataset = np.array(self._dataset)\n",
        "          self._label = np.array(self._label)\n",
        "    def next(self):\n",
        "      if self._cursor + self._batch_size > len(self._dataset):\n",
        "          self._cursor = 0\n",
        "      \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"      \n",
        "      batch_x = self._dataset[self._cursor : self._cursor + self._batch_size,:]\n",
        "      batch_y = self._label[self._cursor : self._cursor + self._batch_size]\n",
        "      self._cursor += self._batch_size\n",
        "      return batch_x, batch_y"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVlmXymbzaFP"
      },
      "source": [
        "best_model = None\n",
        "best_epoch = 0\n",
        "best_acc_val = 0."
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd1LUgSK0cKv"
      },
      "source": [
        "lr = args.learning_rate\n",
        "m = args.margin"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wV3H1xPDUZwU",
        "outputId": "f334ac0d-b16b-40df-ac50-37eb7b0acd21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "acc_per_fold = []\n",
        "precision_per_fold = []\n",
        "recall_per_fold = []\n",
        "f1_per_fold = []\n",
        "\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "\n",
        "fold_no = 1\n",
        "inputs = padded_docs\n",
        "targets = comment_labels\n",
        "\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "  \n",
        "\n",
        "    n_iterations_per_epoch = len(inputs[train]) // args.batch_size\n",
        "    n_iterations_test = len(inputs[test]) // args.batch_size\n",
        "\n",
        "    mr_train1 = BatchGenerator(inputs[train], targets[train], args.batch_size, 0)    \n",
        "    mr_test1 = BatchGenerator(inputs[test], targets[test], args.batch_size, 0, is_shuffle=False)\n",
        "    best_accuracy = 0.\n",
        "    best_precision = 0.\n",
        "    best_recall = 0.\n",
        "    best_f1 = 0.\n",
        "\n",
        "    X = tf.placeholder(tf.int32, [args.batch_size, args.max_sent], name=\"input_x\")\n",
        "    y = tf.placeholder(tf.int64, [args.batch_size, args.num_classes], name=\"input_y\")\n",
        "    is_training = tf.placeholder_with_default(False, shape=())    \n",
        "    learning_rate = tf.placeholder(dtype='float32') \n",
        "    margin = tf.placeholder(shape=(),dtype='float32') \n",
        "\n",
        "    l2_loss = tf.constant(0.0)\n",
        "    w2v = np.array(embedding_vectors,dtype=np.float32)\n",
        "\n",
        "    W1 = tf.Variable(w2v, trainable = False)\n",
        "    X_embedding = tf.nn.embedding_lookup(W1, X)\n",
        "    X_embedding = X_embedding[...,tf.newaxis] \n",
        "\n",
        "\n",
        "    poses, activations = capsule_model_B(X_embedding, args.num_classes)\n",
        "    loss = margin_loss(y, activations) \n",
        "    y_pred = tf.argmax(activations, axis=1, name=\"y_proba\")    \n",
        "    correct = tf.equal(tf.argmax(y, axis=1), y_pred, name=\"correct\")\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name = 'opt'+str(fold_no))   \n",
        "    training_op = optimizer.minimize(loss, name=\"training_op\")\n",
        "    gradients, variables = zip(*optimizer.compute_gradients(loss)) \n",
        "    with tf.Session() as sess:\n",
        "\n",
        "      init = tf.global_variables_initializer()\n",
        "      sess.run(init)\n",
        "\n",
        "      for epoch in range(0,6): \n",
        "\n",
        "        for iteration in range(1, n_iterations_per_epoch + 1):\n",
        "            \n",
        "          \n",
        "          X_batch, y_batch = mr_train1.next()          \n",
        "          _, loss_train, probs, capsule_pose = sess.run(\n",
        "                [training_op, loss, activations, poses],\n",
        "                feed_dict={X: X_batch[:,:args.max_sent],\n",
        "                          y: y_batch,\n",
        "                          is_training: True,\n",
        "                          learning_rate:lr,\n",
        "                          margin:m})        \n",
        "          print(\"\\rIteration: {}/{} ({:.1f}%) epoch:{}  Loss: {:.5f}\".format(iteration, n_iterations_per_epoch, iteration * 100 / n_iterations_per_epoch, epoch+1, loss_train), end=\"\")                        \n",
        "        preds_list, y_list = [], []\n",
        "        for iteration in range(1, n_iterations_test + 1):\n",
        "          X_batch, y_batch = mr_test1.next()             \n",
        "          probs = sess.run([activations],\n",
        "                    feed_dict={X:X_batch[:,:args.max_sent],\n",
        "                                is_training: False})\n",
        "          preds_list = preds_list + probs[0].tolist()\n",
        "          y_list = y_list + y_batch.tolist()\n",
        "\n",
        "        y_list = np.array(y_list)\n",
        "        preds_probs = np.array(preds_list)  \n",
        "        labels = np.argmax(y_list, axis=1)\n",
        "        predictions = np.argmax(preds_probs, axis=1)\n",
        "\n",
        "        accuracy_fold = accuracy_score(labels, predictions)\n",
        "        precision_fold = precision_score(labels, predictions, average='weighted', zero_division = 0 )\n",
        "        recall_fold = recall_score(labels, predictions, average='weighted')\n",
        "        f1_fold = f1_score(labels, predictions, average='weighted')\n",
        "        if best_f1 <= f1_fold :\n",
        "          best_accuracy = accuracy_fold\n",
        "          best_precision = precision_fold\n",
        "          best_recall = recall_fold\n",
        "          best_f1 = f1_fold\n",
        "        \n",
        "\n",
        "      acc_per_fold.append(best_accuracy)\n",
        "      precision_per_fold.append(best_precision)\n",
        "      recall_per_fold.append(best_recall)\n",
        "      f1_per_fold.append(best_f1)\n",
        "      print(\"\\rFold: {} accuracy: {:.4f}%  Precision: {:.4f} recall: {:.4f} F1: {:.4f}\".format(fold_no, best_accuracy, best_precision, best_recall, best_f1))\n",
        "      if args.loss_type == 'margin_loss':    \n",
        "            m = min(0.9, m + 0.1)\n",
        "      fold_no += 1\n",
        "\n",
        "accuracy = np.mean(acc_per_fold)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = np.mean(precision_per_fold)\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = np.mean(recall_per_fold)\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = np.mean(f1_per_fold)\n",
        "print('F1 score: %f' % f1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Iteration: 12/211 (5.7%) epoch:1  Loss: 0.30829"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}